{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e48008b4",
   "metadata": {},
   "source": [
    "### SageMaker  Endpoint 部署ChatGLM\n",
    "  \n",
    "[ChatGLM](https://github.com/THUDM/ChatGLM-6B): ChatGLM-6B 是一个开源的、支持中英双语的对话语言模型，基于 General Language Model (GLM) 架构，具有 62 亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。 ChatGLM-6B 使用了和 ChatGPT 相似的技术，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答。\n",
    "\n",
    "#### 准备\n",
    "1. 升级boto3, sagemaker python sdk  \n",
    "2. 准备inference.py, requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4df976c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.26.91)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.91 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (1.29.91)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.91->boto3) (1.26.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.91->boto3) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.91->boto3) (1.16.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.138.0)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.22.3)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.5.2)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (3.20.3)\n",
      "Requirement already satisfied: attrs<23,>=20.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (22.2.0)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.13.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.28 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.26.91)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.91 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.29.91)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->sagemaker) (3.0.9)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker) (2022.7)\n",
      "Requirement already satisfied: dill>=0.3.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.70.14)\n",
      "Requirement already satisfied: pox>=0.3.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.91->boto3<2.0,>=1.26.28->sagemaker) (1.26.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade boto3\n",
    "!pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d496f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::432088571089:role/AmazonSageMaker-ExecutionRole-20210324T123126\n",
      "sagemaker-us-east-1-432088571089\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region_name = boto3.session.Session().region_name\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(role)\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "564c46fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy\n",
      "upload: ./model.tar.gz to s3://sagemaker-us-east-1-432088571089/chatglm/assets/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!touch dummy\n",
    "!tar czvf model.tar.gz dummy\n",
    "assets_dir = 's3://{0}/{1}/assets/'.format(bucket, 'chatglm')\n",
    "model_data = 's3://{0}/{1}/assets/model.tar.gz'.format(bucket, 'chatglm')\n",
    "!aws s3 cp model.tar.gz $assets_dir\n",
    "!rm -f dummy model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc27746",
   "metadata": {},
   "source": [
    "### 设置模型推理参数\n",
    "1. model_name: Huggingface diffusers models (not support single check point format)\n",
    "2. model_args: diffuser StableDiffusionPipeline init arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bad4ad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = None\n",
    "entry_point = 'inference-chatglm.py'\n",
    "framework_version = '1.13.1'\n",
    "py_version = 'py39'\n",
    "model_environment = {\n",
    "    'SAGEMAKER_MODEL_SERVER_TIMEOUT':'600', \n",
    "    'SAGEMAKER_MODEL_SERVER_WORKERS': '1', \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fc9e17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "model = PyTorchModel(\n",
    "    name = model_name,\n",
    "    model_data = model_data,\n",
    "    entry_point = entry_point,\n",
    "    source_dir = './code',\n",
    "    role = role,\n",
    "    framework_version = framework_version, \n",
    "    py_version = py_version,\n",
    "    env = model_environment\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e3eb04",
   "metadata": {},
   "source": [
    "### 部署 SageMaker Endpoint\n",
    "部署推理服务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7375f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    }
   ],
   "source": [
    "endpoint_name = None\n",
    "instance_type = 'ml.g4dn.2xlarge'\n",
    "instance_count = 1\n",
    "\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "predictor = model.deploy(\n",
    "    endpoint_name = endpoint_name,\n",
    "    instance_type = instance_type, \n",
    "    initial_instance_count = instance_count,\n",
    "    serializer = JSONSerializer(),\n",
    "    deserializer = JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e696ec2",
   "metadata": {},
   "source": [
    "### ChatGLM 测试\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad72190",
   "metadata": {},
   "outputs": [],
   "source": [
    "#休眠2分钟,确保模型可以完全加载\n",
    "import time\n",
    "time.sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccb371c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好👋!我是人工智能助手 ChatGLM-6B,很高兴见到你,欢迎问我任何问题。\n",
      "以下方法是一些可以尝试的来帮助在晚上入睡:\n",
      "\n",
      "1. 创建一个舒适的睡眠环境:保持房间安静、凉爽、黑暗,并使用舒适的床垫和枕头。\n",
      "\n",
      "2. 避免刺激性物质:避免饮用含咖啡因的饮料、饮酒或吸烟。也避免在睡前吃过多的食物或饮用含糖分的饮料。\n",
      "\n",
      "3. 放松自己:尝试进行深呼吸、渐进性肌肉松弛或冥想等放松技巧。也可以听一些轻柔的音乐或进行瑜伽练习来放松自己。\n",
      "\n",
      "4. 制定规律的睡眠时间表:保持每天相同的起床时间和睡眠时间,让身体适应这个规律。\n",
      "\n",
      "5. 避免在床上使用电子设备:避免使用手机、平板电脑或电视等电子设备,因为它们会刺激大脑,使人难以入睡。\n",
      "\n",
      "6. 练习睡前放松技巧:尝试进行冥想或放松瑜伽,帮助身体和大脑放松,准备进入睡眠状态。\n",
      "\n",
      "如果这些方法不能帮助入睡,建议咨询医生或睡眠专家,了解是否有其他问题或需要进一步的帮助。\n"
     ]
    }
   ],
   "source": [
    "inputs= {\n",
    "    \"ask\": \"你好!\"\n",
    "\n",
    "}\n",
    "\n",
    "response = predictor.predict(inputs)\n",
    "print(response[\"answer\"])\n",
    "\n",
    "inputs= {\n",
    "    \"ask\": \"晚上睡不着应该怎么办\"\n",
    "\n",
    "}\n",
    "\n",
    "response = predictor.predict(inputs)\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab5ad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs= {\n",
    "    \"ask\": \"列出一些年夜饭好意头的菜肴以及其寓意!\"\n",
    "\n",
    "}\n",
    "\n",
    "response = predictor.predict(inputs)\n",
    "print(response[\"answer\"])\n",
    "\n",
    "inputs= {\n",
    "    \"ask\": \"帮我写一篇人工智能课程的教案，1000字\"\n",
    "\n",
    "}\n",
    "\n",
    "response = predictor.predict(inputs)\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7b6486c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "要修改Hugging Face Transformers的模型缓存位置,您可以按照以下步骤进行操作:\n",
      "\n",
      "1. 打开Hugging Face Transformers的配置文件(例如`transformers.config`)。\n",
      "\n",
      "2. 在配置文件中找到` cache.size`和` cache.dir`参数。`cache.size`指定了模型缓存的大小,而`cache.dir`指定了模型缓存的位置。\n",
      "\n",
      "3. 修改`cache.dir`参数,为您想要缓存模型的位置创建一个目录。例如,如果您要将模型缓存放在`/path/to/cache/`目录中,则可以将`cache.dir`设置为`/path/to/cache/`。\n",
      "\n",
      "4. 保存并重新加载模型。\n",
      "\n",
      "5. 在加载模型时,您应该会看到模型缓存的位置改变了。\n",
      "\n",
      "例如,假设您要将模型缓存放在`/path/to/cache/my_model_dir`目录中,您可以按照以下步骤修改配置文件:\n",
      "\n",
      "```\n",
      "cache.dir = /path/to/cache/my_model_dir\n",
      "```\n",
      "\n",
      "请注意,`cache.dir`参数必须是一个目录,并且目录名称应该与模型缓存的位置相同。\n",
      "以下是使用 Python 3 编写的快速排序代码示例:\n",
      "\n",
      "```python\n",
      "def quicksort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    else:\n",
      "        pivot = arr[0]\n",
      "        left = []\n",
      "        right = []\n",
      "        for i in range(1, len(arr)):\n",
      "            if arr[i] < pivot:\n",
      "                left.append(arr[i])\n",
      "            else:\n",
      "                right.append(arr[i])\n",
      "        return quicksort(left) + [pivot] + quicksort(right)\n",
      "```\n",
      "\n",
      "这个函数接受一个列表 `arr` 作为输入,并返回一个新的已排序的列表。如果输入列表的长度小于等于 1,则直接返回列表本身。否则,找到第一个元素 `pivot`,并将整个列表分成两个部分:左边的部分的所有元素都小于 `pivot`,右边的部分的所有元素都大于 `pivot`。然后,递归地对左半部分和右半部分执行相同的快速排序操作,并将它们拼接在一起返回。\n",
      "\n",
      "请注意,这个实现是不稳定的,因为在递归调用 `quicksort` 时,可能会将已经排好序的列表重新排序。为了避免这种情况,可以使用一个稳定的实现,例如 `sort` 函数,或者使用随机化选择作为基准值来实现快速排序。\n"
     ]
    }
   ],
   "source": [
    "inputs= {\n",
    "    \"ask\": \"怎么修改huggingface transformers的model cache位置\"\n",
    "\n",
    "}\n",
    "\n",
    "response = predictor.predict(inputs)\n",
    "print(response[\"answer\"])\n",
    "\n",
    "inputs= {\n",
    "    \"ask\": \"用python3写出快速排序代码\"\n",
    "\n",
    "}\n",
    "\n",
    "response = predictor.predict(inputs)\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "932dfec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pytorch-inference-2023-03-15-07-58-44-863'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#我们来查看一下刚部署的这个ChatGLM模型对应的SageMaker inference endpoint\n",
    "sagemaker_endpoint_name = predictor.endpoint_name\n",
    "sagemaker_endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3180d425",
   "metadata": {},
   "source": [
    "利用已经在SageMaker real time inference endpoint部署的ChatGLM模型来模拟单轮对话和多轮对话。\n",
    "\n",
    "下面的代码建议在SageMaker Notebook上来运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1259b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "def query_endpoint_with_json_payload(encoded_json):\n",
    "    response = client.invoke_endpoint(EndpointName=sagemaker_endpoint_name, ContentType='application/json', Body=encoded_json)\n",
    "    return response\n",
    "\n",
    "def parse_response_texts(query_response):\n",
    "    model_predictions = json.loads(query_response['Body'].read())\n",
    "    generated_text = model_predictions[\"answer\"]\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f9b7f0",
   "metadata": {},
   "source": [
    "先简单测试一下ChatGLM针对单个问题的回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7be4ca62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "阿根廷,梅西。\n"
     ]
    }
   ],
   "source": [
    "payload = {\"ask\": \"信息抽取：\\n2022年世界杯的冠军是阿根廷队伍，梅西是MVP\\n问题：国家名，人名\\n答案：\"}\n",
    "query_response = query_endpoint_with_json_payload(json.dumps(payload).encode('utf-8'))\n",
    "generated_texts = parse_response_texts(query_response)\n",
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c3b28b",
   "metadata": {},
   "source": [
    "单轮对话：每个问题/query都是独立的，问题之间没有关联性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "379cad21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户：你好！\n",
      "小元：您好!我是元语AI。我可以回答您的问题、写文章、写作业、翻译，对于一些法律等领域的问题我也可以给你提供信息。\n",
      "用户：今天天气怎么养\n",
      "小元：抱歉,我不太明白您的问题。能否提供更多上下文或信息,让我更好地理解您的问题?谢谢!\n",
      "用户：今天天气怎么？\n",
      "小元：抱歉,作为一个语言模型,我没有实时的天气信息。请查看当地的天气预报或使用天气应用程序来获取当前和未来几天的天气情况。\n",
      "用户：如何下一篇不错的学术论文\n",
      "小元：下一篇不错的学术论文需要以下几个步骤:\n",
      "\n",
      "1. 阅读原始论文:阅读前一篇学术论文是了解当前研究领域和最新研究趋势的关键。因此,应该仔细阅读原始论文,了解作者的研究内容和结论,以及他们的方法、数据和实践。\n",
      "\n",
      "2. 确定目标论文:在阅读原始论文后,需要确定自己想要阅读的下一篇学术论文,以便更好地了解该领域的研究进展和趋势。目标论文应该与自己的研究方向相关,具有创新性和实用性,同时也需要与原始论文进行一定的比较和联系。\n",
      "\n",
      "3. 查找相关文献:在阅读目标论文之前,需要了解该领域的先前研究,以更好地理解作者的研究内容和结论。可以通过图书馆、学术数据库、学术论坛等途径查找相关的文献,以便更好地了解该领域的研究现状和趋势。\n",
      "\n",
      "4. 关注论文质量:一篇好的学术论文应该具有清晰、简洁、准确的表达,合理的结构和逻辑推理,丰富的数据和实证分析,以及清晰、简洁的摘要和结论。因此,在阅读学术论文时,应该注意论文的质量和结构,以便更好地理解和吸收作者的研究成果。\n",
      "\n",
      "5. 与同行交流:与同行交流是了解研究领域和最新研究趋势的重要途径。可以通过学术会议、学术论坛、电子邮件、博客等途径与同行交流,分享自己的想法和研究成果,并从他人的研究成果中学习和借鉴。\n",
      "用户：quit\n"
     ]
    }
   ],
   "source": [
    "#1.首先需要一个简单的开场拍。\n",
    "print(\"用户：你好！\\nChatGLM：您好!我是ChatGLM。我可以回答您的问题、写文章、写作业、翻译，对于一些法律等领域的问题我也可以给你提供信息。\")\n",
    "\n",
    "#2.在同一个session中持续对话，但是每次对话之间没有什么关联。\n",
    "while True:\n",
    "    command = input(\"用户：\")\n",
    "    if command == 'quit':\n",
    "        break;\n",
    "    \n",
    "    #print(command)\n",
    "    payload = {\"ask\": \"\\n用户：\"+ command + \"\\n\"}\n",
    "    #print(payload[\"ask\"])\n",
    "    query_response = query_endpoint_with_json_payload(json.dumps(payload).encode('utf-8'))\n",
    "    generated_texts = \"ChatGLM：\" + parse_response_texts(query_response)\n",
    "    print(generated_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c27beab",
   "metadata": {},
   "source": [
    "多轮对话模拟：我们这里来测试一下ChatGLM的多轮对话能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c1c71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.首先需要开场拍来引导ChatGLM，其实就是给它一个上下文来启动所谓的对话session。\n",
    "payload = {\"ask\": \"用户：你好！\\nChatGLM：您好!我是ChatGLM。我可以回答您的问题、写文章、写作业、翻译，对于一些法律等领域的问题我也可以给你提供信息。\"}\n",
    "print(payload[\"ask\"])\n",
    "generated_texts = \"\"\n",
    "\n",
    "#在这里简单模拟多轮对话时，发送给SageMaker endpoint的payload会越来越大，这里对payload大致做一个限制。\n",
    "session_len = 10 * 1024 * 1024 \n",
    "\n",
    "#2.在同一个session中持续对话，为了有多轮对话的效果，把每一轮的信息(问题-回答对)都带上来告诉ChatGLM之前的上下文。\n",
    "while True:\n",
    "    command = input(\"用户：\")\n",
    "    if command == 'quit':\n",
    "        break;\n",
    "    \n",
    "    #print(command)\n",
    "    whole_context = payload[\"ask\"] + generated_texts + \"\\n用户：\"+ command + \"\\n\"\n",
    "    payload = {\"ask\": whole_context}\n",
    "    if len(whole_context) > session_len:\n",
    "        print(\"上下文信息太多了，当前对话session要退出了！\")\n",
    "        break;\n",
    "    #print(payload[\"ask\"])\n",
    "    query_response = query_endpoint_with_json_payload(json.dumps(payload).encode('utf-8'))\n",
    "    generated_texts = \"ChatGLM：\" + parse_response_texts(query_response)\n",
    "    print(generated_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae617e25",
   "metadata": {},
   "source": [
    "### 删除SageMaker  Endpoint\n",
    "删除推理服务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d1ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5040bf37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
